{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3df2662e-975f-44d5-9342-3273823e8eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OllamaEmbeddings class from the LangChain community embeddings module.\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# Below returns an Ollama embedding model instance using 'mistral' model.\n",
    "def get_embedding_function():\n",
    "    return OllamaEmbeddings(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0855218e-8f3a-4911-b966-52b35a3d2723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ No new passages to add. Everything is already stored.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "embedding_function = get_embedding_function()\n",
    "\n",
    "# --- Step 1: Load passages from JSON ---\n",
    "with open(\"passages.json\", \"r\") as f:\n",
    "    passages = json.load(f)\n",
    "\n",
    "# --- Step 2: Convert each JSON entry into a Document ---\n",
    "new_documents = [\n",
    "    Document(\n",
    "        page_content=p[\"text\"],\n",
    "        metadata={\n",
    "            \"candidate\": p.get(\"candidate\"),\n",
    "            \"timestamp\": p.get(\"timestamp\")\n",
    "        }\n",
    "    )\n",
    "    for p in passages\n",
    "]\n",
    "\n",
    "# --- Step 3: Load existing Chroma database (if any) ---\n",
    "if os.path.exists(CHROMA_PATH):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "    existing = db.get()  # returns dict with 'ids', 'documents', and 'metadatas'\n",
    "    existing_texts = set(existing[\"documents\"]) if existing[\"documents\"] else set()\n",
    "else:\n",
    "    db = Chroma.from_documents([], embedding_function, persist_directory=CHROMA_PATH)\n",
    "    existing_texts = set()\n",
    "\n",
    "# --- Step 4: Filter out duplicates ---\n",
    "unique_documents = [doc for doc in new_documents if doc.page_content not in existing_texts]\n",
    "\n",
    "# --- Step 5: Add only unique documents ---\n",
    "if unique_documents:\n",
    "    db.add_documents(unique_documents)\n",
    "    db.persist()\n",
    "    print(f\"✅ Added {len(unique_documents)} new unique passages.\")\n",
    "else:\n",
    "    print(\"⚙️ No new passages to add. Everything is already stored.\")\n",
    "\n",
    "# --- Optional: Inspect database content ---\n",
    "# all_docs = db.get()\n",
    "# print(all_docs[\"documents\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bca90-7a9b-4959-a669-60ed7aaddac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "## Template for the prompt:\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the given context And provide corresponding candidate and timestamp:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context only: {question}\n",
    "\"\"\"\n",
    "\n",
    "def query_rag(query_text):\n",
    "    # Reload the Chroma vector database from disk\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Retrieves similar passages to the query\n",
    "    results = db.similarity_search_with_score(query_text, k=40)\n",
    "    #print(results)\n",
    "    #print(\"????????\")\n",
    "\n",
    "    # Build context text\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([\n",
    "        f\"{doc.page_content} (Source: {doc.metadata.get('candidate')}, {doc.metadata.get('timestamp')})\"\n",
    "        for doc, _ in results\n",
    "    ])\n",
    "\n",
    "    # Format the prompt using the context and user question\n",
    "    prompt_str = ChatPromptTemplate.from_template(PROMPT_TEMPLATE).format_messages(\n",
    "        context=context_text, question=query_text\n",
    "    )\n",
    "\n",
    "    # Initialize the Ollama model (using the 'mistral' model)\n",
    "    model = Ollama(model=\"mistral\")\n",
    "    # Generate the response from the model\n",
    "    response = model.invoke(prompt_str)\n",
    "\n",
    "    # Collect sources for the retrieved documents\n",
    "    #sources = [\n",
    "     #   f\"{doc.metadata.get('candidate') or 'unknown'} ({doc.metadata.get('timestamp') or 'N/A'})\"\n",
    "      #  for doc, _ in results\n",
    "    #]\n",
    "\n",
    "    # Print the model's response\n",
    "    print(\" Response:\", response)\n",
    "\n",
    "    # FOLLOWING ARE MY TRIALS FOR SOURCE - TO GET CANDIDATE AND TIMESTAMP\n",
    "    #print(\" Sources:\", sources )\n",
    "    #print(results[0][0].metadata[\"candidate\"], results[0][0].metadata[\"timestamp\"])\n",
    "    #unique_sources = {(doc.metadata[\"candidate\"], doc.metadata[\"timestamp\"]) for doc, _ in results}\n",
    "    #unique_sources_list = list(unique_sources)\n",
    "\n",
    "    #print(\" Sources:\")\n",
    "    #for candidate, timestamp in unique_sources:\n",
    "    #last_candidate, last_timestamp = unique_sources_list[-1]\n",
    "        #print(f\"   - {candidate} ({timestamp})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Prompt user for a question and run the RAG query\n",
    "    user_question = input(\" Enter your question: \")\n",
    "    query_rag(user_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81732671-dc27-47aa-bfc6-b78a5a2b3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To restart the database\n",
    "!rm -rf chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6eba8b-b18a-45f4-9aaa-88aa125c739b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
